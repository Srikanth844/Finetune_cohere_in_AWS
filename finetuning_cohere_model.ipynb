{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune and deploy a custom Generative Command model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "* Note: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "* Ensure that IAM role used has AmazonSageMakerFullAccess\n",
    "* To deploy this ML model successfully, ensure that:\n",
    "Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used:\n",
    "aws-marketplace:ViewSubscriptions\n",
    "aws-marketplace:Unsubscribe\n",
    "aws-marketplace:Subscribe\n",
    "or your AWS account has a subscription to the packages for Cohere Command Finetuning. If so, skip step: Subscribe to the finetune algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Subscribe to the finetune algorithm\n",
    "To subscribe to the model algorithm:\n",
    "\n",
    "Open the algorithm listing page Cohere Command Finetuning\n",
    "On the AWS Marketplace listing, click on the Continue to Subscribe button.\n",
    "On the Subscribe to this software page, review and click on \"Accept Offer\" if you and your organization agrees with EULA, pricing, and support terms.\n",
    "Once you click on Continue to configuration button and then choose a region, you will see a Product Arn displayed. This is the algorithm ARN that you need to specify while creating a finetune or deploying the finetuned model as an endpoint using boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U cohere-aws\n",
    "\n",
    "from cohere_aws import Client\n",
    "import boto3\n",
    "import sagemaker as sage\n",
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "cohere_package = \"cohere-command-ft-v2-1-03b5a2b33e113acdbde5f8a142f85db7\"\n",
    "\n",
    "# Mapping for algorithms\n",
    "algorithm_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:algorithm/{cohere_package}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:algorithm/{cohere_package}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:algorithm/{cohere_package}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:algorithm/{cohere_package}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:algorithm/{cohere_package}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:algorithm/{cohere_package}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:algorithm/{cohere_package}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:algorithm/{cohere_package}\",\n",
    "}\n",
    "if region not in algorithm_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "arn = algorithm_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_dir = \"s3://input//\"\n",
    "sess = sage.Session()\n",
    "train_dataset = S3Uploader.upload(\"../Data/finetuning_data.jsonl\", s3_data_dir, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a directory on S3 where finetuned models should be stored. Make sure you do not reuse the same directory across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update this with a custom S3 path\n",
    "# DO NOT re-use the same s3 directory for multiple finetunes\n",
    "# DO NOT add a trailing slash at the end\n",
    "\n",
    "s3_models_dir = \"s3://...\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Cohere Client\n",
    "co = Client(region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parameters = {\n",
    "    \"train_epochs\": 10,\n",
    "    \"strategy\": \"vanilla\",\n",
    "    \"train_batch_size\": 100,\n",
    "    \"early_stopping_patience\": 5,\n",
    "    \"early_stopping_threshold\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take approximately 30 minutes with the example dataset\n",
    "finetune_name = \"sample-finetune\"\n",
    "co.create_finetune(arn=arn,\n",
    "    name=finetune_name,\n",
    "    train_data=train_dataset,\n",
    "    s3_models_dir=s3_models_dir,\n",
    "    instance_type=\"ml.p4de.24xlarge\",\n",
    "    training_parameters=train_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finetuned weights for the above will be store in a tar file {s3_models_dir}/sample-finetune.tar.gz where the file name is the same as the name used during the creation of the finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Endpoint\n",
    "co.create_endpoint(arn=arn,\n",
    "    endpoint_name=\"command-finetune-test\",\n",
    "    s3_models_dir=s3_models_dir,\n",
    "    recreate=True,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    ")\n",
    "\n",
    "# If the endpoint is already created, you just need to connect to it\n",
    "# co.connect_to_endpoint(endpoint_name=\"command-finetune-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform real time inference\n",
    "prompt = \"Classify the following text as either very negative, negative, neutral, positive or very positive: mr. deeds is , as comedy goes , very silly -- and in the best way.\"\n",
    "\n",
    "result = co.generate(prompt=prompt, max_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the endpoint\n",
    "co.delete_endpoint()\n",
    "co.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
